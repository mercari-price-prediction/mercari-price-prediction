{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mercari Price Suggestion Challenge\n",
    "\n",
    "The objective of this challenge is to build an algorithm that automatically suggests the right product prices on Mercari. The training data consists of user-inputted text descriptions of their products, including details like product category name, brand name, and item condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "# Text mining \n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Time \n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def startTime():\n",
    "    return time()\n",
    "def endTime(s):\n",
    "    print \"Time elapsed {}\".format(-s+time())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../../data/train.tsv', sep='\\t')\n",
    "#test = pd.read_csv('../../data/test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# size of training and dataset\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# different data types in the dataset: categorical (strings) and numeric\n",
    "train_df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Overall summary of train data\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First few rows of the dataset \n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retain only part of the data \n",
    "n_samples = 1000\n",
    "train_df = train_df.iloc[:n_samples,:]\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    sent_tokenize(): segment text into sentences\n",
    "    word_tokenize(): break sentences into words\n",
    "    \"\"\"\n",
    "    try: \n",
    "        regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
    "        text = regex.sub(\" \", text) # remove punctuation\n",
    "        \n",
    "        tokens_ = [word_tokenize(s) for s in sent_tokenize(text)]\n",
    "        tokens = []\n",
    "        for token_by_sent in tokens_:\n",
    "            tokens += token_by_sent\n",
    "        tokens = list(filter(lambda t: t.lower() not in stop, tokens))\n",
    "        filtered_tokens = [w for w in tokens if re.search('[a-zA-Z]', w)]\n",
    "        filtered_tokens = [w.lower() for w in filtered_tokens if len(w)>=3]\n",
    "        \n",
    "        return filtered_tokens\n",
    "            \n",
    "    except TypeError as e: print(text,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF...\n",
      "Time elapsed 0.772000074387\n",
      "Extracting tf features for LDA...\n",
      "Time elapsed 0.774000167847\n"
     ]
    }
   ],
   "source": [
    "n_features = 1000\n",
    "\n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=10,\n",
    "                             max_features=180000,\n",
    "                             tokenizer=tokenize,\n",
    "                             ngram_range=(1, 2))\n",
    "\n",
    "t0 = startTime()\n",
    "tfidf = tfidf_vectorizer.fit_transform(train_df['item_description'].apply(str))\n",
    "endTime(t0)\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer_lda = CountVectorizer(min_df=4,max_features=180000,\n",
    "                     tokenizer=tokenize,ngram_range=(1,2))\n",
    "t0 = startTime()\n",
    "tfidf_lda = tf_vectorizer_lda.fit_transform(train_df['item_description'].apply(str))\n",
    "endTime(t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=1000 and n_features=1000...\n",
      "done in 0.175s.\n",
      "\n",
      "Topics in NMF model (Frobenius norm):\n",
      "Topic #0: description yet yet description last large know jeans jacket items item\n",
      "Topic #1: brand brand new new new never sealed firm price firm price leggings new tags\n",
      "Topic #2: size medium size medium like large size small full men black small\n",
      "Topic #3: worn never worn never new never washed times tags material flaws excellent condition\n",
      "Topic #4: free shipping free shipping smoke price home smoke free free home fast firm\n",
      "Topic #5: condition great great condition good good condition times excellent worn excellent condition comes\n",
      "Topic #6: used never used never new never gently used condition eye color opened set\n",
      "Topic #7: cute super pink one black super cute small white top bundle\n",
      "Topic #8: new like new like large new condition authentic bag white times package\n",
      "Topic #9: box new box authentic original without comes set still color case\n",
      "()\n",
      "Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf features, n_samples=1000 and n_features=1000...\n",
      "done in 0.467s.\n",
      "\n",
      "Topics in NMF model (generalized Kullback-Leibler divergence):\n",
      "Topic #0: yet description description yet large know jeans jacket items item free\n",
      "Topic #1: new brand brand new tags size tag made package new tags available\n",
      "Topic #2: size small medium perfect worn black women fit size small size medium\n",
      "Topic #3: worn never never worn used never used new never opened pretty never opened washed\n",
      "Topic #4: shipping free bundle free shipping comes smoke home item save ship\n",
      "Topic #5: condition great great condition good good condition excellent see times excellent condition shape\n",
      "Topic #6: used color full authentic still use little life lot dark\n",
      "Topic #7: cute one top pink white super black soft victoria secret back\n",
      "Topic #8: new like like new silver large gray works white new condition leather\n",
      "Topic #9: box price set firm price firm please new box purchase authentic lip\n",
      "()\n",
      "Fitting LDA models with tf features, n_samples=1000 and n_features=1000...\n",
      "done in 5.690s.\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: sandals size bundle dust tote long sleeve finish animal home sent responsible\n",
      "Topic #1: beauty ask excellent condition 9 months fits like liquid stretchy straps kate spade sexy\n",
      "Topic #2: bundle features chain size 4 feedback size 6 head sides stretchy sandals size\n",
      "Topic #3: pure super soft sure nwt house thanks life left brand new package huge purple\n",
      "Topic #4: purse sandals size old tan perfectly include north crystal noticeable coach\n",
      "Topic #5: long sleeve baseball based ll responsible page banana dust sandals size medium\n",
      "Topic #6: game fast rm rate free home games boot 3rd nwt available\n",
      "Topic #7: clean ultra clear read inches bit tote fabric eyes shoe\n",
      "Topic #8: eyes stretchy eyeshadow life handle let know fun sealed rae dunn outfit\n",
      "Topic #9: pet free home perfect size med instant closure check case long sleeve sandals size sure\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf_frob = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf_frob, tfidf_feature_names, n_top_words)\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
    "      \"tf-idf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf_kld = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss= 'kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf_kld, tfidf_feature_names, n_top_words)\n",
    "\n",
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "\n",
    "n_components = 10\n",
    "n_top_words = 10\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=20,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tfidf_lda)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "tfidf_dict = dict(zip(tfidf_vectorizer.get_feature_names(), tfidf_vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_comp = 20\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(tfidf)\n",
    "train_svd = pd.DataFrame(svd_obj.transform(tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 20)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_svd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nmf_klb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-45ceb6aa138e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlda_trans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_lda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnmf_frob_trans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnmf_frob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnmf_kld_trans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnmf_klb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nmf_klb' is not defined"
     ]
    }
   ],
   "source": [
    "lda_trans = lda.transform(tfidf_lda)\n",
    "nmf_frob_trans = nmf_frob.transform(tfidf)\n",
    "nmf_kld_trans = nmf_klb.transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "plt.scatter(df_train['tfidf'], df_train['price'])\n",
    "plt.title('Train price X item_description TF-IDF', fontsize=15)\n",
    "plt.xlabel('Price', fontsize=15)\n",
    "plt.ylabel('TF-IDF', fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.legend(fontsize=15)\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
