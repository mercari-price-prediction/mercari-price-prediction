{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d04333e758f8716e4768de1b82ba26c8ed038e44"
   },
   "source": [
    "## Mercari Price Suggestion Challenge\n",
    "\n",
    "The objective of this challenge is to build an algorithm that automatically suggests the right product prices on Mercari. The training data consists of user-inputted text descriptions of their products, including details like product category name, brand name, and item condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_cell_guid": "7dfaf2a8-87e1-4b94-828c-7cb4b1e15729",
    "_uuid": "36a7b6e31e04db1369b9e1cbff8e695758a19009",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "import gc\n",
    "import math\n",
    "\n",
    "# Text mining \n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Time \n",
    "from time import time\n",
    "\n",
    "#Plots\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_uuid": "c76413df1793e05f6efd424cb6c079069693837f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def startTime():\n",
    "    return time()\n",
    "def endTime(s):\n",
    "    print (\"Time elapsed {}\".format(time()-s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_uuid": "4ae4bc2ebd32bbdc34beb6f72d6aea646bb5a538"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 8)\n",
      "(40000, 8)\n"
     ]
    }
   ],
   "source": [
    "#df_train = pd.read_csv('../input/train.tsv', sep='\\t')\n",
    "#df_test = pd.read_csv('../input/test.tsv', sep='\\t')\n",
    "\n",
    "df_train = pd.read_csv('../../../../data/train.tsv', sep='\\t')\n",
    "# df_test = pd.read_csv('../../../../data/test.tsv', sep='\\t')\n",
    "n_samples = 100000\n",
    "df_test = df_train[n_samples:n_samples+40000]\n",
    "df_train = df_train[:n_samples]\n",
    "\n",
    "print df_train.shape\n",
    "print df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "837d63f31c9d0f7825c3fd7ac92fd25d473438fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped records where item description was nan\n"
     ]
    }
   ],
   "source": [
    "# Nulls in item description in train or test as tf-idf is not defined on nan\n",
    "# lets drop these 4 items\n",
    "df_train = df_train.loc[df_train.item_description == df_train.item_description]\n",
    "df_train = df_train.loc[df_train.name == df_train.name]\n",
    "print(\"Dropped records where item description was nan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_BRANDS = 4000\n",
    "NUM_CATEGORIES = 1000\n",
    "NAME_MIN_DF = 10\n",
    "\n",
    "def handle_missing_inplace(dataset):\n",
    "    dataset['category_name'].fillna(value='missing', inplace=True)\n",
    "    dataset['brand_name'].fillna(value='missing', inplace=True)\n",
    "    # dataset['item_description'].fillna(value='missing', inplace=True)\n",
    "\n",
    "\n",
    "def cutting(dataset):\n",
    "    pop_brand = dataset['brand_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_BRANDS]\n",
    "    dataset.loc[~dataset['brand_name'].isin(pop_brand), 'brand_name'] = 'missing'\n",
    "    pop_category = dataset['category_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n",
    "    dataset.loc[~dataset['category_name'].isin(pop_category), 'category_name'] = 'missing'\n",
    "\n",
    "\n",
    "def to_categorical(dataset):\n",
    "    dataset['category_name'] = dataset['category_name'].astype('category')\n",
    "    dataset['brand_name'] = dataset['brand_name'].astype('category')\n",
    "    dataset['item_condition_id'] = dataset['item_condition_id'].astype('category')\n",
    "    dataset['shipping'] = dataset['shipping'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed 0.618355035782\n"
     ]
    }
   ],
   "source": [
    "nrow_train = df_train.shape[0]\n",
    "y = np.log1p(df_train[\"price\"])\n",
    "y_test = np.log1p(df_test[\"price\"])\n",
    "merge = pd.concat([df_train, df_test])\n",
    "gc.collect()\n",
    "\n",
    "s = startTime()\n",
    "handle_missing_inplace(merge)\n",
    "cutting(merge)\n",
    "to_categorical(merge)\n",
    "endTime(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed 6.67520809174\n"
     ]
    }
   ],
   "source": [
    "s = startTime()\n",
    "cv = CountVectorizer(min_df=NAME_MIN_DF)\n",
    "X_name = cv.fit_transform(merge['name'])\n",
    "endTime(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed 5.78222703934\n"
     ]
    }
   ],
   "source": [
    "s = startTime()\n",
    "cv = CountVectorizer()\n",
    "X_category = cv.fit_transform(merge['category_name'])\n",
    "endTime(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed 2.33216500282\n"
     ]
    }
   ],
   "source": [
    "s = startTime()\n",
    "lb = LabelBinarizer(sparse_output=True)\n",
    "X_brand = lb.fit_transform(merge['brand_name'])\n",
    "endTime(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed 2.76910305023\n"
     ]
    }
   ],
   "source": [
    "s = startTime()\n",
    "X_dummies = csr_matrix(pd.get_dummies(merge[['item_condition_id', 'shipping']],\n",
    "                                          sparse=True).values)\n",
    "endTime(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparse_merge = hstack((X_dummies, X_brand, X_category, X_name)).tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5df3ae3a8c7655023207f51294703b990b3705ad"
   },
   "source": [
    "## Text Mining : Tf-Idf, NMF, LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Tokenizer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_uuid": "2fd6f90862f80955312fc77b83bbd1b867d79b04",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    sent_tokenize(): segment text into sentences\n",
    "    word_tokenize(): break sentences into words\n",
    "    \"\"\"\n",
    "    try: \n",
    "        regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
    "        text = regex.sub(\" \", text) # remove punctuation\n",
    "        \n",
    "        tokens_ = [word_tokenize(s) for s in sent_tokenize(text)]\n",
    "        tokens = []\n",
    "        for token_by_sent in tokens_:\n",
    "            tokens += token_by_sent\n",
    "        tokens = list(filter(lambda t: t.lower() not in stop, tokens))\n",
    "        filtered_tokens = [w for w in tokens if re.search('[a-zA-Z]', w)]\n",
    "        filtered_tokens = [w.lower() for w in filtered_tokens if len(w)>=3]\n",
    "        \n",
    "        return filtered_tokens\n",
    "            \n",
    "    except TypeError as e: print(text,e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "Time elapsed 177.673354864\n"
     ]
    }
   ],
   "source": [
    "# use raw counts of words\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "#tfidf_vectorizer_lda = CountVectorizer(min_df=20,max_features=50000,\n",
    " #                    tokenizer=tokenize,ngram_range=(1,2))\n",
    "\n",
    "\n",
    "#tfidf_vectorizer_lda = CountVectorizer(max_features = 180000, \n",
    " #                             ngram_range = (1,3),\n",
    "  #                            stop_words = \"english\")\n",
    "\n",
    "tfidf_vectorizer_lda = CountVectorizer(\n",
    "    max_features = 180000,min_df=5, strip_accents='unicode', lowercase =True,\n",
    "    analyzer='word', token_pattern=r'\\w+', ngram_range=(1, 3),\n",
    "    stop_words ='english')\n",
    "\n",
    "t0 = startTime()\n",
    "full_tfidf_lda = tfidf_vectorizer_lda.fit_transform(merge['item_description'].apply(str))\n",
    "train_tfidf_lda = tfidf_vectorizer_lda.transform(df_train['item_description'].apply(str))\n",
    "test_tfidf_lda = tfidf_vectorizer_lda.transform(df_test['item_description'].apply(str))\n",
    "endTime(t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d0289c059f87e45261deba645a7ac1067c613529"
   },
   "source": [
    "### TF-IDF feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_uuid": "6d060bfe95f3b1506e371f7fe4c95060b55278f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF and Normal TFID...\n",
      "Time elapsed 2.50894808769\n"
     ]
    }
   ],
   "source": [
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF and Normal TFID...\")\n",
    "\n",
    "t0 = startTime()\n",
    "tfidf = TfidfTransformer(norm='l2')\n",
    "tfidf.fit(full_tfidf_lda)\n",
    "full_tfidf = tfidf.transform(full_tfidf_lda)\n",
    "train_tfidf = tfidf.transform(train_tfidf_lda)\n",
    "test_tfidf = tfidf.transform(test_tfidf_lda)\n",
    "endTime(t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3b2061da8c9097083a21acb9a5bdf1c87a94d9a8"
   },
   "source": [
    "### SVD on Tf-Idf features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_uuid": "907e0937d9f60218bf679fc6704e8133f3a4fc2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD on TFID to get Latent Representation : k = 25 ...\n",
      "Time elapsed 17.6556529999\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_comp = 25\n",
    "print(\"SVD on TFID to get Latent Representation : k = {} ...\".format(n_comp))\n",
    "t0 = startTime()\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = csr_matrix(svd_obj.transform(train_tfidf))\n",
    "test_svd =  csr_matrix(svd_obj.transform(test_tfidf))\n",
    "endTime(t0)\n",
    "\n",
    "#train_svd.columns = ['svd_item_'+str(i) for i in range(n_comp)]\n",
    "#test_svd.columns = ['svd_item_'+str(i) for i in range(n_comp)]\n",
    "#df_train = pd.concat([df_train, train_svd], axis=1)\n",
    "#df_test = pd.concat([df_test, test_svd], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2624a528f65d5dbb5544e1f130b7bf0f3c822887"
   },
   "source": [
    "* ### NMF - Frobenious Norm,Kullback-Leibler, Divergence, LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_uuid": "20be6d90843c44f1951109d4a627bf709321d878"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=500000 ..\n",
      "Time elapsed 243.305052996\n"
     ]
    }
   ],
   "source": [
    "n_components = 10\n",
    "n_top_words = 10\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "      \"n_samples=%d ..\"\n",
    "      % (n_samples))\n",
    "t0 = startTime()\n",
    "nmf_frob = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(full_tfidf)\n",
    "endTime(t0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
    "      \"tf-idf features, n_samples=%d ..\"\n",
    "      % (n_samples))\n",
    "t0 = time()\n",
    "nmf_kld = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss= 'kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(full_tfidf)\n",
    "endTime(t0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features, n_samples=500000 and ..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-751d96ce5934>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                                 random_state=0)\n\u001b[1;32m      8\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstartTime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_tfidf_lda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mendTime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vgudavar/anaconda2/lib/python2.7/site-packages/sklearn/decomposition/online_lda.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    550\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0midx_slice\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                         self._em_step(X[idx_slice, :], total_samples=n_samples,\n\u001b[0;32m--> 552\u001b[0;31m                                       batch_update=False, parallel=parallel)\n\u001b[0m\u001b[1;32m    553\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                     \u001b[0;31m# batch update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vgudavar/anaconda2/lib/python2.7/site-packages/sklearn/decomposition/online_lda.pyc\u001b[0m in \u001b[0;36m_em_step\u001b[0;34m(self, X, total_samples, batch_update, parallel)\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0;31m# update `component_` related variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         self.exp_dirichlet_component_ = np.exp(\n\u001b[0;32m--> 450\u001b[0;31m             _dirichlet_expectation_2d(self.components_))\n\u001b[0m\u001b[1;32m    451\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_batch_iter_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and ..\"\n",
    "      % (n_samples))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=20,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = startTime()\n",
    "lda.fit(full_tfidf_lda)\n",
    "endTime(t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c92337e2474dd34b52dd8cb9246fafb85dad1538",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_nmf_frob = csr_matrix(nmf_frob.transform(train_tfidf))\n",
    "test_nmf_frob = csr_matrix(nmf_frob.transform(test_tfidf))\n",
    "\n",
    "#train_nmf_kld = csr_matrix(nmf_kld.transform(train_tfidf))\n",
    "#test_nmf_kld = csr_matrix(nmf_kld.transform(test_tfidf))\n",
    "\n",
    "train_lda = csr_matrix(lda.transform(train_tfidf_lda))\n",
    "test_lda = csr_matrix(lda.transform(test_tfidf_lda))\n",
    "\n",
    "\n",
    "#train_nmf_frob_df.columns = ['nmf_frob_'+str(i) for i in range(n_components)]\n",
    "#test_nmf_frob_df.columns = ['nmf_frob_'+str(i) for i in range(n_components)]\n",
    "#df_train = pd.concat([df_train, train_nmf_frob_df], axis=1)\n",
    "#df_test = pd.concat([df_test, test_nmf_frob_df], axis=1)\n",
    "\n",
    "#train_nmf_kld_df.columns = ['nmf_kld_'+str(i) for i in range(n_components)]\n",
    "#test_nmf_kld_df.columns = ['nmf_kld_'+str(i) for i in range(n_components)]\n",
    "#df_train = pd.concat([df_train, train_nmf_kld_df], axis=1)\n",
    "#df_test = pd.concat([df_test, test_nmf_kld_df], axis=1)\n",
    "\n",
    "#train_lda_df.columns = ['lda_'+str(i) for i in range(n_components)]\n",
    "#test_lda_df.columns = ['lda_'+str(i) for i in range(n_components)]\n",
    "#df_train = pd.concat([df_train, train_lda_df], axis=1)\n",
    "#df_test = pd.concat([df_test, test_lda_df], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_uuid": "21ad65fcddf91e437f93caff8cf936fcf155c919"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 8)\n",
      "(1000, 7)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a115c24b37d1d2736ee41f55d76dd6bfa780f58b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_cat = sparse_merge[:nrow_train]\n",
    "X_test_cat = sparse_merge[nrow_train:]\n",
    "#X_train = hstack((X_train_cat, train_svd, train_lda, train_nmf_frob, train_nmf_kld)).tocsr()\n",
    "#X_test = hstack((X_test_cat, test_svd, test_lda, test_nmf_frob, test_nmf_kld)).tocsr()\n",
    "X_train = hstack((X_train_cat, train_svd, train_lda, train_nmf_frob)).tocsr()\n",
    "X_test = hstack((X_test_cat, test_svd, test_lda, test_nmf_frob)).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcMse(yTrue, yPred):\n",
    "    s = 0\n",
    "    for i in range(len(yTrue)):\n",
    "        s += (yTrue[i]-yPred[i])**2\n",
    "    return math.sqrt(s*1.0/len(yTrue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c10d6fe7e6e0ac009931bdc82902c46e21424633"
   },
   "outputs": [],
   "source": [
    "train_X, valid_X, train_y, valid_y = train_test_split(X_train, y, test_size = 0.1, random_state = 144) \n",
    "d_train = lgb.Dataset(train_X, label=train_y, max_bin=8192)\n",
    "d_valid = lgb.Dataset(valid_X, label=valid_y, max_bin=8192)\n",
    "watchlist = [d_train, d_valid]\n",
    "params = {\n",
    "        'learning_rate': 0.75,\n",
    "        'application': 'regression',\n",
    "        'max_depth': 3,\n",
    "        'num_leaves': 50,\n",
    "        'verbosity': -1,\n",
    "        'metric': 'RMSE',\n",
    "}\n",
    "\n",
    "model = lgb.train(params, train_set=d_train, num_boost_round=3600, valid_sets=watchlist, \\\n",
    "early_stopping_rounds=50, verbose_eval=100) \n",
    "preds = 0.55*model.predict(X_test)\n",
    "\n",
    "s = startTime()\n",
    "model = Ridge(solver=\"sag\", fit_intercept=True, random_state=205)\n",
    "model.fit(X_train, y)\n",
    "endTime(s)\n",
    "s = startTime()\n",
    "preds += 0.45*model.predict(X=X_test)\n",
    "endTime(s)\n",
    "y_pred = np.expm1(preds)\n",
    "print  'rmse value is ' + str(calcMse(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
