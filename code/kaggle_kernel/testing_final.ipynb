{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "4ddcb930-8049-4683-88c6-7c910a0ac457",
        "_uuid": "d04333e758f8716e4768de1b82ba26c8ed038e44"
      },
      "cell_type": "markdown",
      "source": "## Mercari Price Suggestion Challenge\n\nThe objective of this challenge is to build an algorithm that automatically suggests the right product prices on Mercari. The training data consists of user-inputted text descriptions of their products, including details like product category name, brand name, and item condition"
    },
    {
      "metadata": {
        "_cell_guid": "7dfaf2a8-87e1-4b94-828c-7cb4b1e15729",
        "_uuid": "36a7b6e31e04db1369b9e1cbff8e695758a19009",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Import required libraries\nimport numpy as np\nimport pandas as pd\nimport string\nimport re\nimport pickle\nimport gc\n\n# Text mining \nimport nltk\nfrom nltk.stem.porter import *\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\nfrom sklearn.feature_extraction import stop_words\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import LabelBinarizer\nfrom scipy.sparse import csr_matrix, hstack\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split, cross_val_score\nimport lightgbm as lgb\n\n# Time \nfrom time import time\n\n#Plots\nimport matplotlib.pyplot as plt",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "ad08acc8-580e-460d-94d9-ea6559224cb0",
        "_uuid": "c76413df1793e05f6efd424cb6c079069693837f",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "def startTime():\n    return time()\ndef endTime(s):\n    print (\"Time elapsed {}\".format(time()-s))",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "ea6a61c0-5940-4500-a5a3-64587d55faeb",
        "_uuid": "4ae4bc2ebd32bbdc34beb6f72d6aea646bb5a538",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "df_train = pd.read_csv('../input/train.tsv', sep='\\t')\ndf_test = pd.read_csv('../input/test.tsv', sep='\\t')\n\n#df_train = pd.read_csv('../../../data/train.tsv', sep='\\t')\n#df_test = pd.read_csv('../../../data/test.tsv', sep='\\t')",
      "execution_count": 28,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "fd544bed-637c-4750-ab04-52aa432d3010",
        "_uuid": "837d63f31c9d0f7825c3fd7ac92fd25d473438fa",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Nulls in item description in train or test as tf-idf is not defined on nan\n# lets drop these 4 items\nprint(df_train.shape)\nprint(df_test.shape)\ndf_train = df_train.loc[df_train.item_description == df_train.item_description]\ndf_test = df_test.loc[df_test.item_description == df_test.item_description]\ndf_train = df_train.loc[df_train.name == df_train.name]\nprint(\"Dropped records where item description was nan\")\nprint(df_train.shape)\nprint(df_test.shape)",
      "execution_count": 30,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "(1482531, 8)\n(693359, 7)\nDropped records where item description was nan\n(1482531, 8)\n(693359, 7)\n"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "7d688517-24de-4e02-bd6d-7cc695f5cd85",
        "_uuid": "882f92fc7533b4f33ce2655191997f346d64191b"
      },
      "cell_type": "markdown",
      "source": "### Creating Categorical Features"
    },
    {
      "metadata": {
        "_cell_guid": "9cee4f5a-0e06-4bc8-8834-8e61233e8da1",
        "_uuid": "b8786cfac1acafa6c644d9b64e82e672873f79c0",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "NUM_BRANDS = 4000\nNUM_CATEGORIES = 1000\nNAME_MIN_DF = 10\n\ndef handle_missing_inplace(dataset):\n    dataset['category_name'].fillna(value='missing', inplace=True)\n    dataset['brand_name'].fillna(value='missing', inplace=True)\n    # dataset['item_description'].fillna(value='missing', inplace=True)\n\n\ndef cutting(dataset):\n    pop_brand = dataset['brand_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_BRANDS]\n    dataset.loc[~dataset['brand_name'].isin(pop_brand), 'brand_name'] = 'missing'\n    pop_category = dataset['category_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n    dataset.loc[~dataset['category_name'].isin(pop_category), 'category_name'] = 'missing'\n\n\ndef to_categorical(dataset):\n    dataset['category_name'] = dataset['category_name'].astype('category')\n    dataset['brand_name'] = dataset['brand_name'].astype('category')\n    dataset['item_condition_id'] = dataset['item_condition_id'].astype('category')\n    dataset['shipping'] = dataset['shipping'].astype('category')",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "777edba5-9146-4214-beeb-8a9a2a2eeeff",
        "_uuid": "5e8c466036c20db4940bb2da812f9f0b097cb553",
        "trusted": true
      },
      "cell_type": "code",
      "source": "nrow_train = df_train.shape[0]\ny = np.log1p(df_train[\"price\"])\nmerge = pd.concat([df_train, df_test])\nsubmission = df_test[['test_id']]\ngc.collect()\n\ns = startTime()\nhandle_missing_inplace(merge)\ncutting(merge)\nto_categorical(merge)\nendTime(s)",
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Time elapsed 7.4856157302856445\n"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "4d2e0185-a5d9-4425-ba07-d5a2fd6d38ea",
        "_uuid": "4c90815889b87274cde2334fb2ac6d682107046c",
        "trusted": true
      },
      "cell_type": "code",
      "source": "s = startTime()\ncv = CountVectorizer(min_df=NAME_MIN_DF)\nX_name = cv.fit_transform(merge['name'])\nendTime(s)",
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Time elapsed 15.007386684417725\n"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "65310091-ef3d-49fa-b56a-be0e95bcdc8a",
        "_uuid": "eb7512e24ae34f4e67b826c00e9cd58f50e1cc14",
        "trusted": true
      },
      "cell_type": "code",
      "source": "s = startTime()\ncv = CountVectorizer()\nX_category = cv.fit_transform(merge['category_name'])\nendTime(s)",
      "execution_count": 8,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Time elapsed 12.96649432182312\n"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "20355f7f-dc28-432a-86d3-7b01198ca758",
        "_uuid": "6a3a731692f1c4edc8547b40f326fa3f02335c25",
        "trusted": true
      },
      "cell_type": "code",
      "source": "s = startTime()\nlb = LabelBinarizer(sparse_output=True)\nX_brand = lb.fit_transform(merge['brand_name'])\nendTime(s)",
      "execution_count": 9,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Time elapsed 11.55437684059143\n"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "f0a98e0d-a6f8-454d-b670-73b45610229e",
        "_uuid": "11f6b08908a96a39ad4c16fb1f768ad1b654b67e",
        "trusted": true
      },
      "cell_type": "code",
      "source": "s = startTime()\nX_dummies = csr_matrix(pd.get_dummies(merge[['item_condition_id', 'shipping']],\n                                          sparse=True).values)\nendTime(s)",
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Time elapsed 10.293352365493774\n"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "35882676-7f38-40e4-9374-3a748390674e",
        "_uuid": "328d1314098fa38cf16458aff9035b7dfb0736b0",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "sparse_merge = hstack((X_dummies, X_brand, X_category, X_name)).tocsr()",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "ea8b9d6c-7254-4ec4-bae5-2d0e7c836cf5",
        "_uuid": "5df3ae3a8c7655023207f51294703b990b3705ad"
      },
      "cell_type": "markdown",
      "source": "## Text Mining : Tf-Idf, NMF, LDA"
    },
    {
      "metadata": {
        "_cell_guid": "14db2c69-fe87-4634-adf4-9a5533e440fe",
        "_uuid": "7b5e05ac682cb6b5744d85a9f639c7640e139382"
      },
      "cell_type": "markdown",
      "source": "### Define Tokenizer Function"
    },
    {
      "metadata": {
        "_cell_guid": "46bf8eb9-3e2e-45f7-b185-5a39a285694d",
        "_uuid": "2fd6f90862f80955312fc77b83bbd1b867d79b04",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "stop = set(stopwords.words('english'))\ndef tokenize(text):\n    \"\"\"\n    sent_tokenize(): segment text into sentences\n    word_tokenize(): break sentences into words\n    \"\"\"\n    try: \n        regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n        text = regex.sub(\" \", text) # remove punctuation\n        \n        tokens_ = [word_tokenize(s) for s in sent_tokenize(text)]\n        tokens = []\n        for token_by_sent in tokens_:\n            tokens += token_by_sent\n        tokens = list(filter(lambda t: t.lower() not in stop, tokens))\n        filtered_tokens = [w for w in tokens if re.search('[a-zA-Z]', w)]\n        filtered_tokens = [w.lower() for w in filtered_tokens if len(w)>=3]\n        \n        return filtered_tokens\n            \n    except TypeError as e: print(text,e)",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "0814e607-4c28-47c1-ab08-631bf950d56c",
        "_uuid": "d0289c059f87e45261deba645a7ac1067c613529"
      },
      "cell_type": "markdown",
      "source": "### TF-IDF feature extraction"
    },
    {
      "metadata": {
        "_cell_guid": "ccef182c-af37-4406-906e-1b053303a20f",
        "_uuid": "6d060bfe95f3b1506e371f7fe4c95060b55278f7",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Use tf-idf features for NMF.\nprint(\"Extracting tf-idf features for NMF and Normal TFID...\")\ntfidf_vectorizer = TfidfVectorizer(min_df=10,\n                             max_features=180000,\n                             tokenizer=tokenize,\n                             ngram_range=(1, 2))\n\nt0 = startTime()\nfull_tfidf = tfidf_vectorizer.fit_transform(merge['item_description'].apply(str))\ntrain_tfidf = tfidf_vectorizer.transform(df_train['item_description'].apply(str))\ntest_tfidf = tfidf_vectorizer.transform(df_test['item_description'].apply(str))\nendTime(t0)",
      "execution_count": 50,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Extracting tf-idf features for NMF and Normal TFID...\nTime elapsed 1445.708969116211\n"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "719c209d-8cad-4d75-8cbe-217a0ba83ce1",
        "_uuid": "3b2061da8c9097083a21acb9a5bdf1c87a94d9a8"
      },
      "cell_type": "markdown",
      "source": "### SVD on Tf-Idf features"
    },
    {
      "metadata": {
        "_cell_guid": "3e871ef9-ba3c-409d-8fc1-1609dec49dd6",
        "_uuid": "907e0937d9f60218bf679fc6704e8133f3a4fc2f",
        "trusted": true
      },
      "cell_type": "code",
      "source": "n_comp = 25\nprint(\"SVD on TFID to get Latent Representation : k = {} ...\".format(n_comp))\nt0 = startTime()\nsvd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\nsvd_obj.fit(full_tfidf)\ntrain_svd = csr_matrix(svd_obj.transform(train_tfidf))\ntest_svd =  csr_matrix(svd_obj.transform(test_tfidf))\nendTime(t0)\n\n#train_svd.columns = ['svd_item_'+str(i) for i in range(n_comp)]\n#test_svd.columns = ['svd_item_'+str(i) for i in range(n_comp)]\n#df_train = pd.concat([df_train, train_svd], axis=1)\n#df_test = pd.concat([df_test, test_svd], axis=1)\n",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "SVD on TFID to get Latent Representation : k = 25 ...\nTime elapsed 84.3632824420929\n"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "4261763a-0ffd-43fe-ab2f-bcf465ddf802",
        "_uuid": "ca9fe7a04719c2e6d5c4f49ab371e1d0016bbfa3"
      },
      "cell_type": "markdown",
      "source": "### LDA feature extraction"
    },
    {
      "metadata": {
        "_cell_guid": "2a78d3ba-fba6-4d1d-9373-f368dbfda391",
        "_uuid": "c97671b66d535b0beb0e6b05cc808defd51b830f",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Use tf (raw term count) features for LDA.\nprint(\"Extracting tf features for LDA...\")\ntfidf_vectorizer_lda = CountVectorizer(min_df=4,max_features=180000,\n                     tokenizer=tokenize,ngram_range=(1,2))\nt0 = startTime()\nfull_tfidf_lda = tfidf_vectorizer_lda.fit_transform(merge['item_description'].apply(str))\ntrain_tfidf_lda = tfidf_vectorizer_lda.transform(df_train['item_description'].apply(str))\ntest_tfidf_lda = tfidf_vectorizer_lda.transform(df_test['item_description'].apply(str))\nendTime(t0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "4c4c8b61-e7d5-401c-a6fc-13626f0827d5",
        "_uuid": "2624a528f65d5dbb5544e1f130b7bf0f3c822887"
      },
      "cell_type": "markdown",
      "source": "* ### NMF - Frobenious Norm,Kullback-Leibler, Divergence, LDA"
    },
    {
      "metadata": {
        "_cell_guid": "b253ccab-d9f9-4dd5-ab4c-fc3ffe425b02",
        "_uuid": "20be6d90843c44f1951109d4a627bf709321d878",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "n_components = 10\nn_top_words = 10\n\n# Fit the NMF model\nprint(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n      \"n_samples=%d ..\"\n      % (n_samples))\nt0 = startTime()\nnmf_frob = NMF(n_components=n_components, random_state=1,\n          alpha=.1, l1_ratio=.5).fit(full_tfidf)\nendTime(t0)\n\n# Fit the NMF model\nprint(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n      \"tf-idf features, n_samples=%d ..\"\n      % (n_samples))\nt0 = time()\nnmf_kld = NMF(n_components=n_components, random_state=1,\n          beta_loss= 'kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n          l1_ratio=.5).fit(full_tfidf)\nendTime(t0)\n\nprint(\"Fitting LDA models with tf features, \"\n      \"n_samples=%d and ..\"\n      % (n_samples))\nlda = LatentDirichletAllocation(n_components=n_components, max_iter=20,\n                                learning_method='online',\n                                learning_offset=50.,\n                                random_state=0)\nt0 = startTime()\nlda.fit(full_tfidf_lda)\nendTime(t0)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "83c566be-e3ff-4b4b-bf04-ee530a3103f2",
        "_uuid": "c92337e2474dd34b52dd8cb9246fafb85dad1538",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_nmf_frob = csr_matrix(nmf_frob.transform(train_tfidf))\ntest_nmf_frob = csr_matrix(nmf_frob.transform(test_tfidf))\n\ntrain_nmf_kld = csr_matrix(nmf_kld.transform(train_tfidf))\ntest_nmf_kld = csr_matrix(nmf_kld.transform(test_tfidf))\n\ntrain_lda = csr_matrix(lda.transform(train_tfidf_lda))\ntest_lda = csr_matrix(lda.transform(test_tfidf_lda))\n\n\n#train_nmf_frob_df.columns = ['nmf_frob_'+str(i) for i in range(n_components)]\n#test_nmf_frob_df.columns = ['nmf_frob_'+str(i) for i in range(n_components)]\n#df_train = pd.concat([df_train, train_nmf_frob_df], axis=1)\n#df_test = pd.concat([df_test, test_nmf_frob_df], axis=1)\n\n#train_nmf_kld_df.columns = ['nmf_kld_'+str(i) for i in range(n_components)]\n#test_nmf_kld_df.columns = ['nmf_kld_'+str(i) for i in range(n_components)]\n#df_train = pd.concat([df_train, train_nmf_kld_df], axis=1)\n#df_test = pd.concat([df_test, test_nmf_kld_df], axis=1)\n\n#train_lda_df.columns = ['lda_'+str(i) for i in range(n_components)]\n#test_lda_df.columns = ['lda_'+str(i) for i in range(n_components)]\n#df_train = pd.concat([df_train, train_lda_df], axis=1)\n#df_test = pd.concat([df_test, test_lda_df], axis=1)\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "d96fb452-a1bf-455b-8020-ec4d45d3056d",
        "_uuid": "21ad65fcddf91e437f93caff8cf936fcf155c919",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(df_train.shape)\nprint(df_test.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "4d52df7e-eac8-4448-893c-c1ad0fb1a554",
        "_uuid": "a115c24b37d1d2736ee41f55d76dd6bfa780f58b",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "X_train_cat = sparse_merge[:nrow_train]\nX_test_cat = sparse_merge[nrow_train:]\nX_train = hstack((X_train_cat, train_svd, train_lda, train_nmf_frob, train_nmf_kld)).tocsr()\nX_test = hstack((X_test_cat, test_svd, test_lda, test_nmf_frob, test_nmf_kld)).tocsr()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "d0c32b5c-5e05-48d1-bde7-c12d1c879de9",
        "_uuid": "c10d6fe7e6e0ac009931bdc82902c46e21424633",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_X, valid_X, train_y, valid_y = train_test_split(X_train, y, test_size = 0.1, random_state = 144) \nd_train = lgb.Dataset(train_X, label=train_y, max_bin=8192)\nd_valid = lgb.Dataset(valid_X, label=valid_y, max_bin=8192)\nwatchlist = [d_train, d_valid]\nparams = {\n        'learning_rate': 0.75,\n        'application': 'regression',\n        'max_depth': 3,\n        'num_leaves': 50,\n        'verbosity': -1,\n        'metric': 'RMSE',\n}\n\nmodel = lgb.train(params, train_set=d_train, num_boost_round=3600, valid_sets=watchlist, \\\nearly_stopping_rounds=50, verbose_eval=100) \npreds = 0.55*model.predict(X_test)\n\ns = startTime()\nmodel = Ridge(solver=\"sag\", fit_intercept=True, random_state=205)\nmodel.fit(X_train, y)\nendTime(s)\ns = startTime()\npreds += 0.45*model.predict(X=X_test)\nendTime(s)\nsubmission['price'] = np.expm1(preds)\nsubmission.to_csv(\"submission_lgbm_ridge_6.csv\", index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "c96dded5-64ed-4497-aabc-285c75ff7271",
        "_uuid": "b31feb9aadf859eca4b1c48069f191d152863f14",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}