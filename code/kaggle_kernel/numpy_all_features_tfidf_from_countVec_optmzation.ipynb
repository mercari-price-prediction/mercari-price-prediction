{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d04333e758f8716e4768de1b82ba26c8ed038e44"
   },
   "source": [
    "## Mercari Price Suggestion Challenge\n",
    "\n",
    "The objective of this challenge is to build an algorithm that automatically suggests the right product prices on Mercari. The training data consists of user-inputted text descriptions of their products, including details like product category name, brand name, and item condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "7dfaf2a8-87e1-4b94-828c-7cb4b1e15729",
    "_uuid": "36a7b6e31e04db1369b9e1cbff8e695758a19009",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "# Text mining \n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Time \n",
    "from time import time\n",
    "\n",
    "#Plots\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "c76413df1793e05f6efd424cb6c079069693837f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def startTime():\n",
    "    return time()\n",
    "def endTime(s):\n",
    "    print (\"Time elapsed {}\".format(time()-s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "4ae4bc2ebd32bbdc34beb6f72d6aea646bb5a538",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_train = pd.read_csv('../input/train.tsv', sep='\\t')\n",
    "#df_test = pd.read_csv('../input/test.tsv', sep='\\t')\n",
    "\n",
    "df_train = pd.read_csv('../../../data/train.tsv', sep='\\t')\n",
    "df_test = pd.read_csv('../../../data/test.tsv', sep='\\t')\n",
    "n_samples = 1000\n",
    "df_train = df_train[:n_samples]\n",
    "df_test = df_test[:n_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_uuid": "837d63f31c9d0f7825c3fd7ac92fd25d473438fa",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped records where item description was nan\n"
     ]
    }
   ],
   "source": [
    "# Nulls in item description in train or test as tf-idf is not defined on nan\n",
    "# lets drop these 4 items\n",
    "df_train = df_train.loc[df_train.item_description == df_train.item_description]\n",
    "df_train = df_train.loc[df_train.name == df_train.name]\n",
    "print(\"Dropped records where item description was nan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_BRANDS = 4000\n",
    "NUM_CATEGORIES = 1000\n",
    "NAME_MIN_DF = 10\n",
    "\n",
    "def handle_missing_inplace(dataset):\n",
    "    dataset['category_name'].fillna(value='missing', inplace=True)\n",
    "    dataset['brand_name'].fillna(value='missing', inplace=True)\n",
    "    # dataset['item_description'].fillna(value='missing', inplace=True)\n",
    "\n",
    "\n",
    "def cutting(dataset):\n",
    "    pop_brand = dataset['brand_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_BRANDS]\n",
    "    dataset.loc[~dataset['brand_name'].isin(pop_brand), 'brand_name'] = 'missing'\n",
    "    pop_category = dataset['category_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n",
    "    dataset.loc[~dataset['category_name'].isin(pop_category), 'category_name'] = 'missing'\n",
    "\n",
    "\n",
    "def to_categorical(dataset):\n",
    "    dataset['category_name'] = dataset['category_name'].astype('category')\n",
    "    dataset['brand_name'] = dataset['brand_name'].astype('category')\n",
    "    dataset['item_condition_id'] = dataset['item_condition_id'].astype('category')\n",
    "    dataset['shipping'] = dataset['shipping'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed 0.0195360183716\n"
     ]
    }
   ],
   "source": [
    "nrow_train = df_train.shape[0]\n",
    "y = np.log1p(df_train[\"price\"])\n",
    "merge = pd.concat([df_train, df_test])\n",
    "submission = df_test[['test_id']]\n",
    "gc.collect()\n",
    "\n",
    "s = startTime()\n",
    "handle_missing_inplace(merge)\n",
    "cutting(merge)\n",
    "to_categorical(merge)\n",
    "endTime(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed 0.0355749130249\n"
     ]
    }
   ],
   "source": [
    "s = startTime()\n",
    "cv = CountVectorizer(min_df=NAME_MIN_DF)\n",
    "X_name = cv.fit_transform(merge['name'])\n",
    "endTime(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed 0.0458700656891\n"
     ]
    }
   ],
   "source": [
    "s = startTime()\n",
    "cv = CountVectorizer()\n",
    "X_category = cv.fit_transform(merge['category_name'])\n",
    "endTime(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed 0.0177891254425\n"
     ]
    }
   ],
   "source": [
    "s = startTime()\n",
    "lb = LabelBinarizer(sparse_output=True)\n",
    "X_brand = lb.fit_transform(merge['brand_name'])\n",
    "endTime(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed 0.0302591323853\n"
     ]
    }
   ],
   "source": [
    "s = startTime()\n",
    "X_dummies = csr_matrix(pd.get_dummies(merge[['item_condition_id', 'shipping']],\n",
    "                                          sparse=True).values)\n",
    "endTime(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparse_merge = hstack((X_dummies, X_brand, X_category, X_name)).tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5df3ae3a8c7655023207f51294703b990b3705ad"
   },
   "source": [
    "## Text Mining : Tf-Idf, NMF, LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Tokenizer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_uuid": "2fd6f90862f80955312fc77b83bbd1b867d79b04",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    sent_tokenize(): segment text into sentences\n",
    "    word_tokenize(): break sentences into words\n",
    "    \"\"\"\n",
    "    try: \n",
    "        regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
    "        text = regex.sub(\" \", text) # remove punctuation\n",
    "        \n",
    "        tokens_ = [word_tokenize(s) for s in sent_tokenize(text)]\n",
    "        tokens = []\n",
    "        for token_by_sent in tokens_:\n",
    "            tokens += token_by_sent\n",
    "        tokens = list(filter(lambda t: t.lower() not in stop, tokens))\n",
    "        filtered_tokens = [w for w in tokens if re.search('[a-zA-Z]', w)]\n",
    "        filtered_tokens = [w.lower() for w in filtered_tokens if len(w)>=3]\n",
    "        \n",
    "        return filtered_tokens\n",
    "            \n",
    "    except TypeError as e: print(text,e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "Time elapsed 1.34153699875\n"
     ]
    }
   ],
   "source": [
    "# use raw counts of words\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tfidf_vectorizer_lda = CountVectorizer(min_df=8,max_features=180000,\n",
    "                     tokenizer=tokenize,ngram_range=(1,2))\n",
    "t0 = startTime()\n",
    "full_tfidf_lda = tfidf_vectorizer_lda.fit_transform(merge['item_description'].apply(str))\n",
    "train_tfidf_lda = tfidf_vectorizer_lda.transform(df_train['item_description'].apply(str))\n",
    "test_tfidf_lda = tfidf_vectorizer_lda.transform(df_test['item_description'].apply(str))\n",
    "endTime(t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d0289c059f87e45261deba645a7ac1067c613529"
   },
   "source": [
    "### TF-IDF feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_uuid": "6d060bfe95f3b1506e371f7fe4c95060b55278f7",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF and Normal TFID...\n",
      "Time elapsed 0.00539588928223\n"
     ]
    }
   ],
   "source": [
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF and Normal TFID...\")\n",
    "\n",
    "t0 = startTime()\n",
    "tfidf = TfidfTransformer(norm='l2')\n",
    "tfidf.fit(full_tfidf_lda)\n",
    "full_tfidf = tfidf.transform(full_tfidf_lda)\n",
    "train_tfidf = tfidf.transform(train_tfidf_lda)\n",
    "test_tfidf = tfidf.transform(test_tfidf_lda)\n",
    "endTime(t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3b2061da8c9097083a21acb9a5bdf1c87a94d9a8"
   },
   "source": [
    "### SVD on Tf-Idf features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_uuid": "907e0937d9f60218bf679fc6704e8133f3a4fc2f",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD on TFID to get Latent Representation : k = 25 ...\n",
      "Time elapsed 0.0743420124054\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_comp = 25\n",
    "print(\"SVD on TFID to get Latent Representation : k = {} ...\".format(n_comp))\n",
    "t0 = startTime()\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = csr_matrix(svd_obj.transform(train_tfidf))\n",
    "test_svd =  csr_matrix(svd_obj.transform(test_tfidf))\n",
    "endTime(t0)\n",
    "\n",
    "#train_svd.columns = ['svd_item_'+str(i) for i in range(n_comp)]\n",
    "#test_svd.columns = ['svd_item_'+str(i) for i in range(n_comp)]\n",
    "#df_train = pd.concat([df_train, train_svd], axis=1)\n",
    "#df_test = pd.concat([df_test, test_svd], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2624a528f65d5dbb5544e1f130b7bf0f3c822887"
   },
   "source": [
    "* ### NMF - Frobenious Norm,Kullback-Leibler, Divergence, LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_uuid": "20be6d90843c44f1951109d4a627bf709321d878",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=1000 ..\n",
      "Time elapsed 0.454634904861\n",
      "Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf features, n_samples=1000 ..\n",
      "Time elapsed 0.48970580101\n",
      "Fitting LDA models with tf features, n_samples=1000 and ..\n",
      "Time elapsed 4.86125302315\n"
     ]
    }
   ],
   "source": [
    "n_components = 10\n",
    "n_top_words = 10\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "      \"n_samples=%d ..\"\n",
    "      % (n_samples))\n",
    "t0 = startTime()\n",
    "nmf_frob = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(full_tfidf)\n",
    "endTime(t0)\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
    "      \"tf-idf features, n_samples=%d ..\"\n",
    "      % (n_samples))\n",
    "t0 = time()\n",
    "nmf_kld = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss= 'kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(full_tfidf)\n",
    "endTime(t0)\n",
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and ..\"\n",
    "      % (n_samples))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=20,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = startTime()\n",
    "lda.fit(full_tfidf_lda)\n",
    "endTime(t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_uuid": "c92337e2474dd34b52dd8cb9246fafb85dad1538",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_nmf_frob = csr_matrix(nmf_frob.transform(train_tfidf))\n",
    "test_nmf_frob = csr_matrix(nmf_frob.transform(test_tfidf))\n",
    "\n",
    "train_nmf_kld = csr_matrix(nmf_kld.transform(train_tfidf))\n",
    "test_nmf_kld = csr_matrix(nmf_kld.transform(test_tfidf))\n",
    "\n",
    "train_lda = csr_matrix(lda.transform(train_tfidf_lda))\n",
    "test_lda = csr_matrix(lda.transform(test_tfidf_lda))\n",
    "\n",
    "\n",
    "#train_nmf_frob_df.columns = ['nmf_frob_'+str(i) for i in range(n_components)]\n",
    "#test_nmf_frob_df.columns = ['nmf_frob_'+str(i) for i in range(n_components)]\n",
    "#df_train = pd.concat([df_train, train_nmf_frob_df], axis=1)\n",
    "#df_test = pd.concat([df_test, test_nmf_frob_df], axis=1)\n",
    "\n",
    "#train_nmf_kld_df.columns = ['nmf_kld_'+str(i) for i in range(n_components)]\n",
    "#test_nmf_kld_df.columns = ['nmf_kld_'+str(i) for i in range(n_components)]\n",
    "#df_train = pd.concat([df_train, train_nmf_kld_df], axis=1)\n",
    "#df_test = pd.concat([df_test, test_nmf_kld_df], axis=1)\n",
    "\n",
    "#train_lda_df.columns = ['lda_'+str(i) for i in range(n_components)]\n",
    "#test_lda_df.columns = ['lda_'+str(i) for i in range(n_components)]\n",
    "#df_train = pd.concat([df_train, train_lda_df], axis=1)\n",
    "#df_test = pd.concat([df_test, test_lda_df], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_uuid": "21ad65fcddf91e437f93caff8cf936fcf155c919",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 8)\n",
      "(1000, 7)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_uuid": "a115c24b37d1d2736ee41f55d76dd6bfa780f58b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_cat = sparse_merge[:nrow_train]\n",
    "X_test_cat = sparse_merge[nrow_train:]\n",
    "X_train = hstack((X_train_cat, train_svd, train_lda, train_nmf_frob, train_nmf_kld)).tocsr()\n",
    "X_test = hstack((X_test_cat, test_svd, test_lda, test_nmf_frob, test_nmf_kld)).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_uuid": "c10d6fe7e6e0ac009931bdc82902c46e21424633",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[11]\ttraining's rmse: 0.566273\tvalid_1's rmse: 0.667757\n",
      "Time elapsed 0.0391399860382\n",
      "Time elapsed 0.000572919845581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vgudavar/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "train_X, valid_X, train_y, valid_y = train_test_split(X_train, y, test_size = 0.1, random_state = 144) \n",
    "d_train = lgb.Dataset(train_X, label=train_y, max_bin=8192)\n",
    "d_valid = lgb.Dataset(valid_X, label=valid_y, max_bin=8192)\n",
    "watchlist = [d_train, d_valid]\n",
    "params = {\n",
    "        'learning_rate': 0.75,\n",
    "        'application': 'regression',\n",
    "        'max_depth': 3,\n",
    "        'num_leaves': 50,\n",
    "        'verbosity': -1,\n",
    "        'metric': 'RMSE',\n",
    "}\n",
    "\n",
    "model = lgb.train(params, train_set=d_train, num_boost_round=3600, valid_sets=watchlist, \\\n",
    "early_stopping_rounds=50, verbose_eval=100) \n",
    "preds = 0.55*model.predict(X_test)\n",
    "\n",
    "s = startTime()\n",
    "model = Ridge(solver=\"sag\", fit_intercept=True, random_state=205)\n",
    "model.fit(X_train, y)\n",
    "endTime(s)\n",
    "s = startTime()\n",
    "preds += 0.45*model.predict(X=X_test)\n",
    "endTime(s)\n",
    "submission['price'] = np.expm1(preds)\n",
    "submission.to_csv(\"submission_lgbm_ridge_6.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
