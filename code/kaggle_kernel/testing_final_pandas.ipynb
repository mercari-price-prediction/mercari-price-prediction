{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4ddcb930-8049-4683-88c6-7c910a0ac457",
    "_uuid": "d04333e758f8716e4768de1b82ba26c8ed038e44"
   },
   "source": [
    "## Mercari Price Suggestion Challenge\n",
    "\n",
    "The objective of this challenge is to build an algorithm that automatically suggests the right product prices on Mercari. The training data consists of user-inputted text descriptions of their products, including details like product category name, brand name, and item condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "7dfaf2a8-87e1-4b94-828c-7cb4b1e15729",
    "_uuid": "36a7b6e31e04db1369b9e1cbff8e695758a19009",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "# Text mining \n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Time \n",
    "from time import time\n",
    "\n",
    "#Plots\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ad08acc8-580e-460d-94d9-ea6559224cb0",
    "_uuid": "c76413df1793e05f6efd424cb6c079069693837f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def startTime():\n",
    "    return time()\n",
    "def endTime(s):\n",
    "    print (\"Time elapsed {}\".format(time()-s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_cell_guid": "ea6a61c0-5940-4500-a5a3-64587d55faeb",
    "_uuid": "4ae4bc2ebd32bbdc34beb6f72d6aea646bb5a538",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_train = pd.read_csv('../input/train.tsv', sep='\\t')\n",
    "#df_test = pd.read_csv('../input/test.tsv', sep='\\t')\n",
    "\n",
    "\n",
    "df_train = pd.read_csv('../../../data/train.tsv', sep='\\t')\n",
    "df_test = pd.read_csv('../../../data/test.tsv', sep='\\t')\n",
    "\n",
    "df_train = df_train[n_samplpes:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_cell_guid": "fd544bed-637c-4750-ab04-52aa432d3010",
    "_uuid": "837d63f31c9d0f7825c3fd7ac92fd25d473438fa",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1482531, 8)\n",
      "(693359, 7)\n",
      "Dropped records where item description was nan\n",
      "(1482531, 8)\n",
      "(693359, 7)\n"
     ]
    }
   ],
   "source": [
    "# Nulls in item description in train or test as tf-idf is not defined on nan\n",
    "# lets drop these 4 items\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "df_train = df_train.loc[df_train.item_description == df_train.item_description]\n",
    "df_test = df_test.loc[df_test.item_description == df_test.item_description]\n",
    "df_train = df_train.loc[df_train.name == df_train.name]\n",
    "print(\"Dropped records where item description was nan\")\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7d688517-24de-4e02-bd6d-7cc695f5cd85",
    "_uuid": "882f92fc7533b4f33ce2655191997f346d64191b"
   },
   "source": [
    "### Creating Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "9cee4f5a-0e06-4bc8-8834-8e61233e8da1",
    "_uuid": "b8786cfac1acafa6c644d9b64e82e672873f79c0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_BRANDS = 4000\n",
    "NUM_CATEGORIES = 1000\n",
    "NAME_MIN_DF = 10\n",
    "\n",
    "def handle_missing_inplace(dataset):\n",
    "    dataset['category_name'].fillna(value='missing', inplace=True)\n",
    "    dataset['brand_name'].fillna(value='missing', inplace=True)\n",
    "    # dataset['item_description'].fillna(value='missing', inplace=True)\n",
    "\n",
    "\n",
    "def cutting(dataset):\n",
    "    pop_brand = dataset['brand_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_BRANDS]\n",
    "    dataset.loc[~dataset['brand_name'].isin(pop_brand), 'brand_name'] = 'missing'\n",
    "    pop_category = dataset['category_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n",
    "    dataset.loc[~dataset['category_name'].isin(pop_category), 'category_name'] = 'missing'\n",
    "\n",
    "\n",
    "def to_categorical(dataset):\n",
    "    dataset['category_name'] = dataset['category_name'].astype('category')\n",
    "    dataset['brand_name'] = dataset['brand_name'].astype('category')\n",
    "    dataset['item_condition_id'] = dataset['item_condition_id'].astype('category')\n",
    "    dataset['shipping'] = dataset['shipping'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "777edba5-9146-4214-beeb-8a9a2a2eeeff",
    "_uuid": "5e8c466036c20db4940bb2da812f9f0b097cb553",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed 7.4856157302856445\n"
     ]
    }
   ],
   "source": [
    "nrow_train = df_train.shape[0]\n",
    "y = np.log1p(df_train[\"price\"])\n",
    "merge = pd.concat([df_train, df_test])\n",
    "submission = df_test[['test_id']]\n",
    "gc.collect()\n",
    "\n",
    "s = startTime()\n",
    "handle_missing_inplace(merge)\n",
    "cutting(merge)\n",
    "to_categorical(merge)\n",
    "endTime(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "4d2e0185-a5d9-4425-ba07-d5a2fd6d38ea",
    "_uuid": "4c90815889b87274cde2334fb2ac6d682107046c",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed 15.007386684417725\n"
     ]
    }
   ],
   "source": [
    "s = startTime()\n",
    "cv = CountVectorizer(min_df=NAME_MIN_DF)\n",
    "X_name = cv.fit_transform(merge['name'])\n",
    "endTime(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "65310091-ef3d-49fa-b56a-be0e95bcdc8a",
    "_uuid": "eb7512e24ae34f4e67b826c00e9cd58f50e1cc14",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed 12.96649432182312\n"
     ]
    }
   ],
   "source": [
    "s = startTime()\n",
    "cv = CountVectorizer()\n",
    "X_category = cv.fit_transform(merge['category_name'])\n",
    "endTime(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "20355f7f-dc28-432a-86d3-7b01198ca758",
    "_uuid": "6a3a731692f1c4edc8547b40f326fa3f02335c25",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed 11.55437684059143\n"
     ]
    }
   ],
   "source": [
    "s = startTime()\n",
    "lb = LabelBinarizer(sparse_output=True)\n",
    "X_brand = lb.fit_transform(merge['brand_name'])\n",
    "endTime(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "f0a98e0d-a6f8-454d-b670-73b45610229e",
    "_uuid": "11f6b08908a96a39ad4c16fb1f768ad1b654b67e",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed 10.293352365493774\n"
     ]
    }
   ],
   "source": [
    "s = startTime()\n",
    "X_dummies = csr_matrix(pd.get_dummies(merge[['item_condition_id', 'shipping']],\n",
    "                                          sparse=True).values)\n",
    "endTime(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "35882676-7f38-40e4-9374-3a748390674e",
    "_uuid": "328d1314098fa38cf16458aff9035b7dfb0736b0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparse_merge = hstack((X_dummies, X_brand, X_category, X_name)).tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ea8b9d6c-7254-4ec4-bae5-2d0e7c836cf5",
    "_uuid": "5df3ae3a8c7655023207f51294703b990b3705ad"
   },
   "source": [
    "## Text Mining : Tf-Idf, NMF, LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "14db2c69-fe87-4634-adf4-9a5533e440fe",
    "_uuid": "7b5e05ac682cb6b5744d85a9f639c7640e139382"
   },
   "source": [
    "### Define Tokenizer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "46bf8eb9-3e2e-45f7-b185-5a39a285694d",
    "_uuid": "2fd6f90862f80955312fc77b83bbd1b867d79b04",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    sent_tokenize(): segment text into sentences\n",
    "    word_tokenize(): break sentences into words\n",
    "    \"\"\"\n",
    "    try: \n",
    "        regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
    "        text = regex.sub(\" \", text) # remove punctuation\n",
    "        \n",
    "        tokens_ = [word_tokenize(s) for s in sent_tokenize(text)]\n",
    "        tokens = []\n",
    "        for token_by_sent in tokens_:\n",
    "            tokens += token_by_sent\n",
    "        tokens = list(filter(lambda t: t.lower() not in stop, tokens))\n",
    "        filtered_tokens = [w for w in tokens if re.search('[a-zA-Z]', w)]\n",
    "        filtered_tokens = [w.lower() for w in filtered_tokens if len(w)>=3]\n",
    "        \n",
    "        return filtered_tokens\n",
    "            \n",
    "    except TypeError as e: print(text,e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0814e607-4c28-47c1-ab08-631bf950d56c",
    "_uuid": "d0289c059f87e45261deba645a7ac1067c613529"
   },
   "source": [
    "### TF-IDF feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_cell_guid": "ccef182c-af37-4406-906e-1b053303a20f",
    "_uuid": "6d060bfe95f3b1506e371f7fe4c95060b55278f7",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF and Normal TFID...\n",
      "Time elapsed 1445.708969116211\n"
     ]
    }
   ],
   "source": [
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF and Normal TFID...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=10,\n",
    "                             max_features=180000,\n",
    "                             tokenizer=tokenize,\n",
    "                             ngram_range=(1, 2))\n",
    "\n",
    "t0 = startTime()\n",
    "full_tfidf = tfidf_vectorizer.fit_transform(merge['item_description'].apply(str))\n",
    "train_tfidf = tfidf_vectorizer.transform(df_train['item_description'].apply(str))\n",
    "test_tfidf = tfidf_vectorizer.transform(df_test['item_description'].apply(str))\n",
    "endTime(t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "719c209d-8cad-4d75-8cbe-217a0ba83ce1",
    "_uuid": "3b2061da8c9097083a21acb9a5bdf1c87a94d9a8"
   },
   "source": [
    "### SVD on Tf-Idf features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3e871ef9-ba3c-409d-8fc1-1609dec49dd6",
    "_uuid": "907e0937d9f60218bf679fc6704e8133f3a4fc2f",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD on TFID to get Latent Representation : k = 25 ...\n",
      "Time elapsed 84.3632824420929\n"
     ]
    }
   ],
   "source": [
    "n_comp = 25\n",
    "print(\"SVD on TFID to get Latent Representation : k = {} ...\".format(n_comp))\n",
    "t0 = startTime()\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = csr_matrix(svd_obj.transform(train_tfidf))\n",
    "test_svd =  csr_matrix(svd_obj.transform(test_tfidf))\n",
    "endTime(t0)\n",
    "\n",
    "#train_svd.columns = ['svd_item_'+str(i) for i in range(n_comp)]\n",
    "#test_svd.columns = ['svd_item_'+str(i) for i in range(n_comp)]\n",
    "#df_train = pd.concat([df_train, train_svd], axis=1)\n",
    "#df_test = pd.concat([df_test, test_svd], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4261763a-0ffd-43fe-ab2f-bcf465ddf802",
    "_uuid": "ca9fe7a04719c2e6d5c4f49ab371e1d0016bbfa3"
   },
   "source": [
    "### LDA feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2a78d3ba-fba6-4d1d-9373-f368dbfda391",
    "_uuid": "c97671b66d535b0beb0e6b05cc808defd51b830f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tfidf_vectorizer_lda = CountVectorizer(min_df=4,max_features=180000,\n",
    "                     tokenizer=tokenize,ngram_range=(1,2))\n",
    "t0 = startTime()\n",
    "full_tfidf_lda = tfidf_vectorizer_lda.fit_transform(merge['item_description'].apply(str))\n",
    "train_tfidf_lda = tfidf_vectorizer_lda.transform(df_train['item_description'].apply(str))\n",
    "test_tfidf_lda = tfidf_vectorizer_lda.transform(df_test['item_description'].apply(str))\n",
    "endTime(t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4c4c8b61-e7d5-401c-a6fc-13626f0827d5",
    "_uuid": "2624a528f65d5dbb5544e1f130b7bf0f3c822887"
   },
   "source": [
    "* ### NMF - Frobenious Norm,Kullback-Leibler, Divergence, LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b253ccab-d9f9-4dd5-ab4c-fc3ffe425b02",
    "_uuid": "20be6d90843c44f1951109d4a627bf709321d878",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_components = 10\n",
    "n_top_words = 10\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "      \"n_samples=%d ..\"\n",
    "      % (n_samples))\n",
    "t0 = startTime()\n",
    "nmf_frob = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(full_tfidf)\n",
    "endTime(t0)\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
    "      \"tf-idf features, n_samples=%d ..\"\n",
    "      % (n_samples))\n",
    "t0 = time()\n",
    "nmf_kld = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss= 'kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(full_tfidf)\n",
    "endTime(t0)\n",
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and ..\"\n",
    "      % (n_samples))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=20,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = startTime()\n",
    "lda.fit(full_tfidf_lda)\n",
    "endTime(t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "83c566be-e3ff-4b4b-bf04-ee530a3103f2",
    "_uuid": "c92337e2474dd34b52dd8cb9246fafb85dad1538",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_nmf_frob = csr_matrix(nmf_frob.transform(train_tfidf))\n",
    "test_nmf_frob = csr_matrix(nmf_frob.transform(test_tfidf))\n",
    "\n",
    "train_nmf_kld = csr_matrix(nmf_kld.transform(train_tfidf))\n",
    "test_nmf_kld = csr_matrix(nmf_kld.transform(test_tfidf))\n",
    "\n",
    "train_lda = csr_matrix(lda.transform(train_tfidf_lda))\n",
    "test_lda = csr_matrix(lda.transform(test_tfidf_lda))\n",
    "\n",
    "\n",
    "#train_nmf_frob_df.columns = ['nmf_frob_'+str(i) for i in range(n_components)]\n",
    "#test_nmf_frob_df.columns = ['nmf_frob_'+str(i) for i in range(n_components)]\n",
    "#df_train = pd.concat([df_train, train_nmf_frob_df], axis=1)\n",
    "#df_test = pd.concat([df_test, test_nmf_frob_df], axis=1)\n",
    "\n",
    "#train_nmf_kld_df.columns = ['nmf_kld_'+str(i) for i in range(n_components)]\n",
    "#test_nmf_kld_df.columns = ['nmf_kld_'+str(i) for i in range(n_components)]\n",
    "#df_train = pd.concat([df_train, train_nmf_kld_df], axis=1)\n",
    "#df_test = pd.concat([df_test, test_nmf_kld_df], axis=1)\n",
    "\n",
    "#train_lda_df.columns = ['lda_'+str(i) for i in range(n_components)]\n",
    "#test_lda_df.columns = ['lda_'+str(i) for i in range(n_components)]\n",
    "#df_train = pd.concat([df_train, train_lda_df], axis=1)\n",
    "#df_test = pd.concat([df_test, test_lda_df], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d96fb452-a1bf-455b-8020-ec4d45d3056d",
    "_uuid": "21ad65fcddf91e437f93caff8cf936fcf155c919",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4d52df7e-eac8-4448-893c-c1ad0fb1a554",
    "_uuid": "a115c24b37d1d2736ee41f55d76dd6bfa780f58b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_cat = sparse_merge[:nrow_train]\n",
    "X_test_cat = sparse_merge[nrow_train:]\n",
    "X_train = hstack((X_train_cat, train_svd, train_lda, train_nmf_frob, train_nmf_kld)).tocsr()\n",
    "X_test = hstack((X_test_cat, test_svd, test_lda, test_nmf_frob, test_nmf_kld)).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d0c32b5c-5e05-48d1-bde7-c12d1c879de9",
    "_uuid": "c10d6fe7e6e0ac009931bdc82902c46e21424633",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X, valid_X, train_y, valid_y = train_test_split(X_train, y, test_size = 0.1, random_state = 144) \n",
    "d_train = lgb.Dataset(train_X, label=train_y, max_bin=8192)\n",
    "d_valid = lgb.Dataset(valid_X, label=valid_y, max_bin=8192)\n",
    "watchlist = [d_train, d_valid]\n",
    "params = {\n",
    "        'learning_rate': 0.75,\n",
    "        'application': 'regression',\n",
    "        'max_depth': 3,\n",
    "        'num_leaves': 50,\n",
    "        'verbosity': -1,\n",
    "        'metric': 'RMSE',\n",
    "}\n",
    "\n",
    "model = lgb.train(params, train_set=d_train, num_boost_round=3600, valid_sets=watchlist, \\\n",
    "early_stopping_rounds=50, verbose_eval=100) \n",
    "preds = 0.55*model.predict(X_test)\n",
    "\n",
    "s = startTime()\n",
    "model = Ridge(solver=\"sag\", fit_intercept=True, random_state=205)\n",
    "model.fit(X_train, y)\n",
    "endTime(s)\n",
    "s = startTime()\n",
    "preds += 0.45*model.predict(X=X_test)\n",
    "endTime(s)\n",
    "submission['price'] = np.expm1(preds)\n",
    "submission.to_csv(\"submission_lgbm_ridge_6.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c96dded5-64ed-4497-aabc-285c75ff7271",
    "_uuid": "b31feb9aadf859eca4b1c48069f191d152863f14",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
