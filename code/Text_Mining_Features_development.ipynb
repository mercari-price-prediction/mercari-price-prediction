{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d04333e758f8716e4768de1b82ba26c8ed038e44"
   },
   "source": [
    "## Mercari Price Suggestion Challenge\n",
    "\n",
    "The objective of this challenge is to build an algorithm that automatically suggests the right product prices on Mercari. The training data consists of user-inputted text descriptions of their products, including details like product category name, brand name, and item condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_cell_guid": "7dfaf2a8-87e1-4b94-828c-7cb4b1e15729",
    "_uuid": "36a7b6e31e04db1369b9e1cbff8e695758a19009"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "# Text mining \n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Time \n",
    "from time import time\n",
    "\n",
    "#Plots\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "c76413df1793e05f6efd424cb6c079069693837f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def startTime():\n",
    "    return time()\n",
    "def endTime(s):\n",
    "    print (\"Time elapsed {}\".format(time()-s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "4ae4bc2ebd32bbdc34beb6f72d6aea646bb5a538"
   },
   "outputs": [],
   "source": [
    "#df_train = pd.read_csv('../input/train.tsv', sep='\\t')\n",
    "#df_test = pd.read_csv('../input/test.tsv', sep='\\t')\n",
    "\n",
    "df_train = pd.read_csv('../../../data/train.tsv', sep='\\t')\n",
    "df_test = pd.read_csv('../../../data/test.tsv', sep='\\t')\n",
    "\n",
    "# Retain only part of the data \n",
    "n_samples = 10000\n",
    "df_train = df_train.iloc[:n_samples,:]\n",
    "df_test = df_test.iloc[:n_samples,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "837d63f31c9d0f7825c3fd7ac92fd25d473438fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped records where item description was nan\n"
     ]
    }
   ],
   "source": [
    "# Nulls in item description in train or test as tf-idf is not defined on nan\n",
    "# lets drop these 4 items\n",
    "df_train = df_train.loc[df_train.item_description == df_train.item_description]\n",
    "df_train = df_train.loc[df_train.name == df_train.name]\n",
    "print(\"Dropped records where item description was nan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_BRANDS = 4000\n",
    "NUM_CATEGORIES = 1000\n",
    "NAME_MIN_DF = 10\n",
    "\n",
    "def handle_missing_inplace(dataset):\n",
    "    dataset['category_name'].fillna(value='missing', inplace=True)\n",
    "    dataset['brand_name'].fillna(value='missing', inplace=True)\n",
    "    # dataset['item_description'].fillna(value='missing', inplace=True)\n",
    "\n",
    "\n",
    "def cutting(dataset):\n",
    "    pop_brand = dataset['brand_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_BRANDS]\n",
    "    dataset.loc[~dataset['brand_name'].isin(pop_brand), 'brand_name'] = 'missing'\n",
    "    pop_category = dataset['category_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n",
    "    dataset.loc[~dataset['category_name'].isin(pop_category), 'category_name'] = 'missing'\n",
    "\n",
    "\n",
    "def to_categorical(dataset):\n",
    "    dataset['category_name'] = dataset['category_name'].astype('category')\n",
    "    dataset['brand_name'] = dataset['brand_name'].astype('category')\n",
    "    dataset['item_condition_id'] = dataset['item_condition_id'].astype('category')\n",
    "    dataset['shipping'] = dataset['shipping'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed 0.173171043396\n",
      "[None] Finished to handle missing\n",
      "Time elapsed 0.0727500915527\n",
      "[None] Finished to cut\n",
      "Time elapsed 0.0482790470123\n",
      "[None] Finished to convert categorical\n"
     ]
    }
   ],
   "source": [
    "s = startTime()\n",
    "handle_missing_inplace(df_train)\n",
    "handle_missing_inplace(df_test)\n",
    "print('[{}] Finished to handle missing'.format(endTime(s)))\n",
    "\n",
    "s = startTime()\n",
    "cutting(df_train)\n",
    "cutting(df_test)\n",
    "print('[{}] Finished to cut'.format(endTime(s)))\n",
    "\n",
    "s = startTime()\n",
    "to_categorical(df_train)\n",
    "to_categorical(df_test)\n",
    "print('[{}] Finished to convert categorical'.format(endTime(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# product name related features \n",
    "s = startTime()\n",
    "tf_vec = CountVectorizer(stop_words='english', ngram_range=(1,1))\n",
    "full_tf = tf_vec.fit_transform(df_train['name'].values.tolist() + df_test['name'].values.tolist())\n",
    "train_tf = tf_vec.transform(df_train['name'].values.tolist())\n",
    "test_tf = tf_vec.transform(df_test['name'].values.tolist())\n",
    "\n",
    "n_comp = 40\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tf.astype('float32'))\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tf))\n",
    "    \n",
    "train_svd.columns = ['svd_name_'+str(i) for i in range(n_comp)]\n",
    "test_svd.columns = ['svd_name_'+str(i) for i in range(n_comp)]\n",
    "df_train = pd.concat([df_train, train_svd], axis=1)\n",
    "df_test = pd.concat([df_test, test_svd], axis=1)\n",
    "endTime(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed 0.509783029556\n"
     ]
    }
   ],
   "source": [
    "s = startTime()\n",
    "tf_vec = CountVectorizer(stop_words='english', ngram_range=(1,1))\n",
    "full_tf = tf_vec.fit_transform(df_train['category_name'].values.tolist() + df_test['category_name'].values.tolist())\n",
    "train_tf = tf_vec.transform(df_train['category_name'].values.tolist())\n",
    "test_tf = tf_vec.transform(df_test['category_name'].values.tolist())\n",
    "\n",
    "n_comp = 40\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tf.astype('float32'))\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tf))\n",
    "    \n",
    "train_svd.columns = ['svd_category_name_'+str(i) for i in range(n_comp)]\n",
    "test_svd.columns = ['svd_category_name_'+str(i) for i in range(n_comp)]\n",
    "df_train = pd.concat([df_train, train_svd], axis=1)\n",
    "df_test = pd.concat([df_test, test_svd], axis=1)\n",
    "endTime(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Categorical cannot perform the operation +",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-64ab7924b297>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstartTime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbrand_Vctzr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'brand_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'brand_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mbrand_trainVctzr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'brand_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbrand_testVctzr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'brand_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vgudavar/anaconda2/lib/python2.7/site-packages/pandas/core/ops.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(left, right, name, na_op)\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0mlvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrap_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_na_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m         return construct_result(\n\u001b[1;32m    741\u001b[0m             \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vgudavar/anaconda2/lib/python2.7/site-packages/pandas/core/ops.pyc\u001b[0m in \u001b[0;36msafe_na_op\u001b[0;34m(lvalues, rvalues)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mna_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vgudavar/anaconda2/lib/python2.7/site-packages/pandas/core/ops.pyc\u001b[0m in \u001b[0;36mna_op\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    688\u001b[0m                 raise TypeError(\"{typ} cannot perform the operation \"\n\u001b[1;32m    689\u001b[0m                                 \"{op}\".format(typ=type(x).__name__,\n\u001b[0;32m--> 690\u001b[0;31m                                               op=str_rep))\n\u001b[0m\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchanged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_upcast_putmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Categorical cannot perform the operation +"
     ]
    }
   ],
   "source": [
    "s = startTime()\n",
    "lb = LabelBinarizer(sparse_output=True)\n",
    "brand_Vctzr = lb.fit_transform(df_train['brand_name'] + df_test['brand_name'])\n",
    "brand_trainVctzr = pd.DataFrame(lb.transform(df_train['brand_name'].apply(str)))\n",
    "brand_testVctzr = pd.DataFrame(lb.transform(df_test['brand_name'].apply(str)))\n",
    "print('[{}] Finished label binarize `brand_name`'.format(time.time() - s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "s = startTime()\n",
    "nrow_train = df_train.shape[0]\n",
    "print nrow_train\n",
    "merge_df = pd.concat([df_train, df_test])\n",
    "item_shipping_dummies = (pd.get_dummies(merge_df[['item_condition_id', 'shipping']],\n",
    "                                         sparse=True).values)\n",
    "print (item_shipping_dummies)\n",
    "print('[{}] Finished to get dummies on `item_condition_id` and `shipping`'.format(endTime(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1153)\n"
     ]
    }
   ],
   "source": [
    "print a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5df3ae3a8c7655023207f51294703b990b3705ad"
   },
   "source": [
    "## Text Mining : Tf-Idf, NMF, LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Tokenizer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2fd6f90862f80955312fc77b83bbd1b867d79b04",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    sent_tokenize(): segment text into sentences\n",
    "    word_tokenize(): break sentences into words\n",
    "    \"\"\"\n",
    "    try: \n",
    "        regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
    "        text = regex.sub(\" \", text) # remove punctuation\n",
    "        \n",
    "        tokens_ = [word_tokenize(s) for s in sent_tokenize(text)]\n",
    "        tokens = []\n",
    "        for token_by_sent in tokens_:\n",
    "            tokens += token_by_sent\n",
    "        tokens = list(filter(lambda t: t.lower() not in stop, tokens))\n",
    "        filtered_tokens = [w for w in tokens if re.search('[a-zA-Z]', w)]\n",
    "        filtered_tokens = [w.lower() for w in filtered_tokens if len(w)>=3]\n",
    "        \n",
    "        return filtered_tokens\n",
    "            \n",
    "    except TypeError as e: print(text,e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d0289c059f87e45261deba645a7ac1067c613529"
   },
   "source": [
    "### TF-IDF feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6d060bfe95f3b1506e371f7fe4c95060b55278f7"
   },
   "outputs": [],
   "source": [
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF and Normal TFID...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=10,\n",
    "                             max_features=180000,\n",
    "                             tokenizer=tokenize,\n",
    "                             ngram_range=(1, 2))\n",
    "\n",
    "t0 = startTime()\n",
    "full_tfidf = tfidf_vectorizer.fit_transform(df_train['item_description'].apply(str) + df_test['item_description'].apply(str))\n",
    "train_tfidf = tfidf_vectorizer.transform(df_train['item_description'].apply(str))\n",
    "test_tfidf = tfidf_vectorizer.transform(df_test['item_description'].apply(str))\n",
    "endTime(t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3b2061da8c9097083a21acb9a5bdf1c87a94d9a8"
   },
   "source": [
    "### SVD on Tf-Idf features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "907e0937d9f60218bf679fc6704e8133f3a4fc2f"
   },
   "outputs": [],
   "source": [
    "\n",
    "n_comp = 25\n",
    "print(\"SVD on TFID to get Latent Representation : k = {} ...\".format(n_comp))\n",
    "t0 = startTime()\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "endTime(t0)\n",
    "\n",
    "train_svd.columns = ['svd_item_'+str(i) for i in range(n_comp)]\n",
    "test_svd.columns = ['svd_item_'+str(i) for i in range(n_comp)]\n",
    "df_train = pd.concat([df_train, train_svd], axis=1)\n",
    "df_test = pd.concat([df_test, test_svd], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ca9fe7a04719c2e6d5c4f49ab371e1d0016bbfa3"
   },
   "source": [
    "### LDA feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c97671b66d535b0beb0e6b05cc808defd51b830f"
   },
   "outputs": [],
   "source": [
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tfidf_vectorizer_lda = CountVectorizer(min_df=4,max_features=180000,\n",
    "                     tokenizer=tokenize,ngram_range=(1,2))\n",
    "t0 = startTime()\n",
    "full_tfidf_lda = tfidf_vectorizer_lda.fit_transform(df_train['item_description'].apply(str) + df_test['item_description'].apply(str))\n",
    "train_tfidf_lda = tfidf_vectorizer_lda.transform(df_train['item_description'].apply(str))\n",
    "test_tfidf_lda = tfidf_vectorizer_lda.transform(df_test['item_description'].apply(str))\n",
    "endTime(t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2624a528f65d5dbb5544e1f130b7bf0f3c822887"
   },
   "source": [
    "* ### NMF - Frobenious Norm,Kullback-Leibler, Divergence, LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "20be6d90843c44f1951109d4a627bf709321d878"
   },
   "outputs": [],
   "source": [
    "n_components = 10\n",
    "n_top_words = 10\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "      \"n_samples=%d ..\"\n",
    "      % (n_samples))\n",
    "t0 = startTime()\n",
    "nmf_frob = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(full_tfidf)\n",
    "endTime(t0)\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
    "      \"tf-idf features, n_samples=%d ..\"\n",
    "      % (n_samples))\n",
    "t0 = time()\n",
    "nmf_kld = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss= 'kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(full_tfidf)\n",
    "endTime(t0)\n",
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and ..\"\n",
    "      % (n_samples))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=20,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = startTime()\n",
    "lda.fit(full_tfidf_lda)\n",
    "endTime(t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c92337e2474dd34b52dd8cb9246fafb85dad1538",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_nmf_frob_df = pd.DataFrame(nmf_frob.transform(train_tfidf))\n",
    "test_nmf_frob_df = pd.DataFrame(nmf_frob.transform(test_tfidf))\n",
    "\n",
    "train_nmf_kld_df = pd.DataFrame(nmf_kld.transform(train_tfidf))\n",
    "test_nmf_kld_df = pd.DataFrame(nmf_kld.transform(test_tfidf))\n",
    "\n",
    "train_lda_df = pd.DataFrame(lda.transform(train_tfidf_lda))\n",
    "test_lda_df = pd.DataFrame(lda.transform(test_tfidf_lda))\n",
    "\n",
    "\n",
    "train_nmf_frob_df.columns = ['nmf_frob_'+str(i) for i in range(n_components)]\n",
    "test_nmf_frob_df.columns = ['nmf_frob_'+str(i) for i in range(n_components)]\n",
    "df_train = pd.concat([df_train, train_nmf_frob_df], axis=1)\n",
    "df_test = pd.concat([df_test, test_nmf_frob_df], axis=1)\n",
    "\n",
    "train_nmf_kld_df.columns = ['nmf_kld_'+str(i) for i in range(n_components)]\n",
    "test_nmf_kld_df.columns = ['nmf_kld_'+str(i) for i in range(n_components)]\n",
    "df_train = pd.concat([df_train, train_nmf_kld_df], axis=1)\n",
    "df_test = pd.concat([df_test, test_nmf_kld_df], axis=1)\n",
    "\n",
    "train_lda_df.columns = ['lda_'+str(i) for i in range(n_components)]\n",
    "test_lda_df.columns = ['lda_'+str(i) for i in range(n_components)]\n",
    "df_train = pd.concat([df_train, train_lda_df], axis=1)\n",
    "df_test = pd.concat([df_test, test_lda_df], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "21ad65fcddf91e437f93caff8cf936fcf155c919"
   },
   "outputs": [],
   "source": [
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a115c24b37d1d2736ee41f55d76dd6bfa780f58b"
   },
   "outputs": [],
   "source": [
    "df_train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c10d6fe7e6e0ac009931bdc82902c46e21424633",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
